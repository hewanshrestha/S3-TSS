{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform EuroSat classification using VIT\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vision_transformer as vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "init = [None,\"IMAGENET1K_V1\", \"IMAGENET1K_SWAG_E2E_V1\"]\n",
    "backbone = models.vit_b_16\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "training_type = ['fine_tuning', 'linear_probe']\n",
    "batch_size = 64  \n",
    "num_epochs = 20\n",
    "lr = 0.001  \n",
    "weight_decay = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_percentage_of_train_dataset(dataset, percentage):\n",
    "    class_counts = {}\n",
    "    indices_by_class = {}\n",
    "    # Count the number of samples in each class\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        if label not in class_counts:\n",
    "            class_counts[label] = 0\n",
    "            indices_by_class[label] = []\n",
    "        class_counts[label] += 1\n",
    "        indices_by_class[label].append(idx)\n",
    "\n",
    "    selected_indices = []\n",
    "\n",
    "    # Select a percentage of samples from each class\n",
    "    for _, indices in indices_by_class.items():\n",
    "        num_samples = int(len(indices) * percentage)\n",
    "        selected_samples = random.sample(indices, num_samples)\n",
    "        selected_indices.extend(selected_samples)\n",
    "\n",
    "    selected_data = Subset(dataset, selected_indices)\n",
    "    return selected_data\n",
    "\n",
    "def model_def(backbone, training_type, weights, num_classes, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    model = backbone(weights=weights)\n",
    "    model.heads.head = nn.Linear(in_features=model.heads.head.in_features, out_features=num_classes)\n",
    "\n",
    "    if training_type == 'fine_tuning':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif training_type == 'linear_probe':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.heads.head.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError('Invalid training type!')\n",
    "    return model.to(device)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()  \n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if device:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return accuracy, test_loss\n",
    "\n",
    "def train_model(model, selected_train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, save_dir):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_acc = 0.0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f'Epoch {epoch} of {num_epochs}')\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "        iter_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        model.train()  \n",
    "\n",
    "        for i, (inputs, labels) in enumerate(selected_train_loader):\n",
    "            if device:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            iter_loss += loss.item() \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss.append(iter_loss / iterations)\n",
    "        train_accuracy.append(100 * correct / total)\n",
    "\n",
    "        test_acc, test_losses = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        test_loss.append(test_losses)\n",
    "        test_accuracy.append(test_acc)\n",
    "\n",
    "        print(f'Epoch {epoch} Train results: Loss={train_loss[-1]:.4f} | Accuracy={train_accuracy[-1]:.2f}%')\n",
    "        print(f'Epoch {epoch} Test results: Loss={test_loss[-1]:.4f} | Accuracy={test_accuracy[-1]:.2f}%\\n')\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            # torch.save(model.state_dict(), os.path.join(save_dir, 'best_acc_model.pth'))\n",
    "\n",
    "        if test_losses < best_loss:\n",
    "            best_loss = test_losses\n",
    "            # torch.save(model.state_dict(), os.path.join(save_dir, 'best_loss_model.pth'))\n",
    "    return train_loss, train_accuracy, test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/akansh-i2sc/Desktop/Study/HLCV/Why-Self-Supervision-in-Time/data/EuroSAT/\"\n",
    "#Data transforms and dataloaders\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'train'), data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'test'), data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "class_names = train_dataset.classes\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 300, 1: 300, 2: 300, 3: 250, 4: 250, 5: 200, 6: 250, 7: 300, 8: 250, 9: 300}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(dict(Counter(test_dataset.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_list = np.arange(0.0, 1.1, 0.10)\n",
    "percentage_list[0] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45036e66dece428082bbf450c04370b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b60c37009f41239fb0b339fa4ce05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine_tuning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d575dc0be21a42cdb869b76a8753ea9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the dataset used: 0.01\n",
      "Epoch 1 of 20\n",
      "Epoch 1 Train results: Loss=4.5064 | Accuracy=10.37%\n",
      "Epoch 1 Test results: Loss=4.3440 | Accuracy=11.11%\n",
      "\n",
      "Epoch 2 of 20\n",
      "Epoch 2 Train results: Loss=3.4440 | Accuracy=8.30%\n",
      "Epoch 2 Test results: Loss=2.3632 | Accuracy=11.11%\n",
      "\n",
      "Epoch 3 of 20\n",
      "Epoch 3 Train results: Loss=2.5623 | Accuracy=11.20%\n",
      "Epoch 3 Test results: Loss=2.6363 | Accuracy=11.11%\n",
      "\n",
      "Epoch 4 of 20\n",
      "Epoch 4 Train results: Loss=2.4923 | Accuracy=15.35%\n",
      "Epoch 4 Test results: Loss=2.2773 | Accuracy=19.48%\n",
      "\n",
      "Epoch 5 of 20\n",
      "Epoch 5 Train results: Loss=2.2921 | Accuracy=14.94%\n",
      "Epoch 5 Test results: Loss=2.2985 | Accuracy=11.11%\n",
      "\n",
      "Epoch 6 of 20\n",
      "Epoch 6 Train results: Loss=2.2656 | Accuracy=13.28%\n",
      "Epoch 6 Test results: Loss=2.2080 | Accuracy=9.74%\n",
      "\n",
      "Epoch 7 of 20\n",
      "Epoch 7 Train results: Loss=2.1789 | Accuracy=17.43%\n",
      "Epoch 7 Test results: Loss=2.1437 | Accuracy=19.85%\n",
      "\n",
      "Epoch 8 of 20\n",
      "Epoch 8 Train results: Loss=2.1238 | Accuracy=21.16%\n",
      "Epoch 8 Test results: Loss=2.1337 | Accuracy=20.19%\n",
      "\n",
      "Epoch 9 of 20\n",
      "Epoch 9 Train results: Loss=2.1044 | Accuracy=21.58%\n",
      "Epoch 9 Test results: Loss=2.1159 | Accuracy=22.00%\n",
      "\n",
      "Epoch 10 of 20\n",
      "Epoch 10 Train results: Loss=2.0813 | Accuracy=24.07%\n",
      "Epoch 10 Test results: Loss=2.0991 | Accuracy=21.30%\n",
      "\n",
      "Epoch 11 of 20\n",
      "Epoch 11 Train results: Loss=2.0662 | Accuracy=24.07%\n",
      "Epoch 11 Test results: Loss=2.0848 | Accuracy=21.81%\n",
      "\n",
      "Epoch 12 of 20\n",
      "Epoch 12 Train results: Loss=2.0530 | Accuracy=24.48%\n",
      "Epoch 12 Test results: Loss=2.0741 | Accuracy=21.96%\n",
      "\n",
      "Epoch 13 of 20\n",
      "Epoch 13 Train results: Loss=2.0436 | Accuracy=23.24%\n",
      "Epoch 13 Test results: Loss=2.0648 | Accuracy=22.00%\n",
      "\n",
      "Epoch 14 of 20\n",
      "Epoch 14 Train results: Loss=2.0288 | Accuracy=24.07%\n",
      "Epoch 14 Test results: Loss=2.0548 | Accuracy=22.04%\n",
      "\n",
      "Epoch 15 of 20\n",
      "Epoch 15 Train results: Loss=2.0199 | Accuracy=23.65%\n",
      "Epoch 15 Test results: Loss=2.0538 | Accuracy=22.07%\n",
      "\n",
      "Epoch 16 of 20\n",
      "Epoch 16 Train results: Loss=2.0200 | Accuracy=23.24%\n",
      "Epoch 16 Test results: Loss=2.0525 | Accuracy=22.11%\n",
      "\n",
      "Epoch 17 of 20\n",
      "Epoch 17 Train results: Loss=2.0194 | Accuracy=23.24%\n",
      "Epoch 17 Test results: Loss=2.0513 | Accuracy=22.07%\n",
      "\n",
      "Epoch 18 of 20\n",
      "Epoch 18 Train results: Loss=2.0193 | Accuracy=23.24%\n",
      "Epoch 18 Test results: Loss=2.0499 | Accuracy=22.11%\n",
      "\n",
      "Epoch 19 of 20\n",
      "Epoch 19 Train results: Loss=2.0162 | Accuracy=23.65%\n",
      "Epoch 19 Test results: Loss=2.0486 | Accuracy=22.15%\n",
      "\n",
      "Epoch 20 of 20\n",
      "Epoch 20 Train results: Loss=2.0172 | Accuracy=23.65%\n",
      "Epoch 20 Test results: Loss=2.0473 | Accuracy=22.19%\n",
      "\n",
      "Percentage of the dataset used: 0.1\n",
      "Epoch 1 of 20\n",
      "Epoch 1 Train results: Loss=2.4733 | Accuracy=20.00%\n",
      "Epoch 1 Test results: Loss=2.0270 | Accuracy=22.07%\n",
      "\n",
      "Epoch 2 of 20\n",
      "Epoch 2 Train results: Loss=2.0722 | Accuracy=19.55%\n",
      "Epoch 2 Test results: Loss=2.0062 | Accuracy=22.89%\n",
      "\n",
      "Epoch 3 of 20\n",
      "Epoch 3 Train results: Loss=1.9975 | Accuracy=25.84%\n",
      "Epoch 3 Test results: Loss=1.9707 | Accuracy=28.93%\n",
      "\n",
      "Epoch 4 of 20\n",
      "Epoch 4 Train results: Loss=1.9807 | Accuracy=24.61%\n",
      "Epoch 4 Test results: Loss=1.8080 | Accuracy=30.26%\n",
      "\n",
      "Epoch 5 of 20\n",
      "Epoch 5 Train results: Loss=2.0434 | Accuracy=25.51%\n",
      "Epoch 5 Test results: Loss=2.2991 | Accuracy=11.48%\n",
      "\n",
      "Epoch 6 of 20\n",
      "Epoch 6 Train results: Loss=2.0463 | Accuracy=23.00%\n",
      "Epoch 6 Test results: Loss=1.8235 | Accuracy=30.96%\n",
      "\n",
      "Epoch 7 of 20\n",
      "Epoch 7 Train results: Loss=1.7843 | Accuracy=31.69%\n",
      "Epoch 7 Test results: Loss=1.7433 | Accuracy=32.52%\n",
      "\n",
      "Epoch 8 of 20\n",
      "Epoch 8 Train results: Loss=1.6934 | Accuracy=35.47%\n",
      "Epoch 8 Test results: Loss=1.6625 | Accuracy=36.59%\n",
      "\n",
      "Epoch 9 of 20\n",
      "Epoch 9 Train results: Loss=1.6498 | Accuracy=38.11%\n",
      "Epoch 9 Test results: Loss=1.6298 | Accuracy=37.63%\n",
      "\n",
      "Epoch 10 of 20\n",
      "Epoch 10 Train results: Loss=1.6301 | Accuracy=37.16%\n",
      "Epoch 10 Test results: Loss=1.6256 | Accuracy=36.78%\n",
      "\n",
      "Epoch 11 of 20\n",
      "Epoch 11 Train results: Loss=1.6112 | Accuracy=38.19%\n",
      "Epoch 11 Test results: Loss=1.6099 | Accuracy=35.93%\n",
      "\n",
      "Epoch 12 of 20\n",
      "Epoch 12 Train results: Loss=1.5978 | Accuracy=38.48%\n",
      "Epoch 12 Test results: Loss=1.6028 | Accuracy=37.22%\n",
      "\n",
      "Epoch 13 of 20\n",
      "Epoch 13 Train results: Loss=1.5827 | Accuracy=37.90%\n",
      "Epoch 13 Test results: Loss=1.5648 | Accuracy=37.00%\n",
      "\n",
      "Epoch 14 of 20\n",
      "Epoch 14 Train results: Loss=1.5533 | Accuracy=39.30%\n",
      "Epoch 14 Test results: Loss=1.5327 | Accuracy=39.89%\n",
      "\n",
      "Epoch 15 of 20\n",
      "Epoch 15 Train results: Loss=1.5150 | Accuracy=41.44%\n",
      "Epoch 15 Test results: Loss=1.5145 | Accuracy=40.81%\n",
      "\n",
      "Epoch 16 of 20\n",
      "Epoch 16 Train results: Loss=1.4984 | Accuracy=42.22%\n",
      "Epoch 16 Test results: Loss=1.5035 | Accuracy=40.89%\n",
      "\n",
      "Epoch 17 of 20\n",
      "Epoch 17 Train results: Loss=1.4931 | Accuracy=41.89%\n",
      "Epoch 17 Test results: Loss=1.4964 | Accuracy=41.07%\n",
      "\n",
      "Epoch 18 of 20\n",
      "Epoch 18 Train results: Loss=1.4895 | Accuracy=42.76%\n",
      "Epoch 18 Test results: Loss=1.4914 | Accuracy=40.74%\n",
      "\n",
      "Epoch 19 of 20\n",
      "Epoch 19 Train results: Loss=1.4808 | Accuracy=42.30%\n",
      "Epoch 19 Test results: Loss=1.4853 | Accuracy=41.11%\n",
      "\n",
      "Epoch 20 of 20\n",
      "Epoch 20 Train results: Loss=1.4752 | Accuracy=43.00%\n",
      "Epoch 20 Test results: Loss=1.4841 | Accuracy=41.15%\n",
      "\n",
      "Percentage of the dataset used: 0.2\n",
      "Epoch 1 of 20\n",
      "Epoch 1 Train results: Loss=2.3472 | Accuracy=20.86%\n",
      "Epoch 1 Test results: Loss=1.8257 | Accuracy=29.30%\n",
      "\n",
      "Epoch 2 of 20\n",
      "Epoch 2 Train results: Loss=1.7546 | Accuracy=32.04%\n",
      "Epoch 2 Test results: Loss=1.6133 | Accuracy=35.67%\n",
      "\n",
      "Epoch 3 of 20\n",
      "Epoch 3 Train results: Loss=1.6485 | Accuracy=37.37%\n",
      "Epoch 3 Test results: Loss=1.6022 | Accuracy=39.04%\n",
      "\n",
      "Epoch 4 of 20\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"/home/idisc02/Desktop/Hewan/hlcv/Why-Self-Supervision-in-Time/src/models\"\n",
    "train_acc_dict_percentage = {}\n",
    "test_acc_dict_percentage = {}\n",
    "train_loss_dict_percentage = {}\n",
    "test_loss_dict_percentage = {}\n",
    "\n",
    "for k in tqdm(init):\n",
    "    print(k)\n",
    "    for j in tqdm(training_type):\n",
    "        print(j)\n",
    "        for i in tqdm(range(len(percentage_list))):\n",
    "            print(f'Percentage of the dataset used: {percentage_list[i]}')\n",
    "            model = model_def(backbone, j, k, num_classes, device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "            selected_train_dataset = select_percentage_of_train_dataset(train_dataset, percentage_list[i])\n",
    "            selected_train_loader = DataLoader(selected_train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "            train_loss, train_accuracy, test_loss, test_accuracy = train_model(model, selected_train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, save_dir)\n",
    "\n",
    "            train_acc_dict_percentage[f\"{k}_{j}_{percentage_list[i]}\"] = train_accuracy\n",
    "            test_acc_dict_percentage[f\"{k}_{j}_{percentage_list[i]}\"] = test_accuracy\n",
    "            train_loss_dict_percentage[f\"{k}_{j}_{percentage_list[i]}\"] = train_loss\n",
    "            test_loss_dict_percentage[f\"{k}_{j}_{percentage_list[i]}\"] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
